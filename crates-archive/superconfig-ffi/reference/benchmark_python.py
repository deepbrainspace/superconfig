#!/usr/bin/env python3
"""
Performance Benchmark: Python FFI vs NAPI vs WASI
Tests startup time and operation speed differences across all SuperConfig FFI variants
"""

import time
import statistics
import subprocess
import sys
import json
from pathlib import Path

try:
    import superconfig_ffi

    PYTHON_AVAILABLE = True
except ImportError:
    PYTHON_AVAILABLE = False
    print("âŒ Python bindings not available - make sure superconfig_ffi.so exists")


class PythonBenchmark:
    def __init__(self):
        self.results = {"startup": [], "operations": []}

    def measure_time_ns(self, func):
        """Measure time in nanoseconds for high precision"""
        start = time.perf_counter_ns()
        result = func()
        end = time.perf_counter_ns()
        return {
            "result": result,
            "time_ns": end - start,
            "time_ms": (end - start) / 1_000_000,
        }

    def benchmark_python_startup(self, iterations=50):
        """Benchmark Python module import + instance creation"""
        print("  ğŸ“Š Testing Python startup time...")

        for i in range(iterations):

            def startup_test():
                # Simulate fresh import by removing from cache
                if "superconfig_ffi" in sys.modules:
                    del sys.modules["superconfig_ffi"]
                import superconfig_ffi

                return superconfig_ffi.SuperConfig()

            measurement = self.measure_time_ns(startup_test)
            self.results["startup"].append(measurement["time_ns"])

    def benchmark_python_operations(self, iterations=1000):
        """Benchmark Python operations (instance creation + method calls)"""
        print("  âš¡ Testing Python operation speed...")

        for i in range(iterations):

            def operation_test():
                config = superconfig_ffi.SuperConfig()
                config.get_verbosity()
                debug_config = config.with_debug_verbosity()
                debug_verbosity = debug_config.get_verbosity()
                return debug_verbosity

            measurement = self.measure_time_ns(operation_test)
            self.results["operations"].append(measurement["time_ns"])

    def calculate_stats(self, times):
        """Calculate comprehensive statistics"""
        if not times:
            return None

        times_ms = [t / 1_000_000 for t in times]  # Convert to milliseconds
        times_ms.sort()
        n = len(times_ms)

        return {
            "count": n,
            "min": min(times_ms),
            "max": max(times_ms),
            "avg": statistics.mean(times_ms),
            "median": statistics.median(times_ms),
            "p95": times_ms[int(n * 0.95)] if n > 0 else 0,
            "p99": times_ms[int(n * 0.99)] if n > 0 else 0,
            "stddev": statistics.stdev(times_ms) if n > 1 else 0,
        }

    def run_benchmark(self):
        """Run the complete Python benchmark"""
        print("ğŸ Benchmarking Python FFI version...")

        if not PYTHON_AVAILABLE:
            return None

        # Warmup
        for _ in range(5):
            config = superconfig_ffi.SuperConfig()
            config.get_verbosity()

        self.benchmark_python_startup()
        self.benchmark_python_operations()

        startup_stats = self.calculate_stats(self.results["startup"])
        operations_stats = self.calculate_stats(self.results["operations"])

        print("  âœ… Python benchmark completed")

        return {
            "startup": startup_stats,
            "operations": operations_stats,
            "file_size_kb": 681,  # From previous measurements
        }


def run_nodejs_benchmark():
    """Run the Node.js benchmark and return results"""
    print("ğŸŸ¢ Running Node.js benchmark...")

    try:
        # Create a simplified Node.js benchmark script
        nodejs_script = """
const { createRequire } = require('module');
const { hrtime } = require('process');
const require = createRequire(__filename);

function measureTime(fn) {
    const start = hrtime.bigint();
    const result = fn();
    const end = hrtime.bigint();
    return Number(end - start) / 1_000_000; // Convert to ms
}

async function benchmark() {
    const results = { napi: {}, wasi: {} };
    
    // NAPI Benchmark
    try {
        const { SuperConfig } = require('./superconfig_ffi.node');
        
        // Startup benchmark
        const startupTimes = [];
        for (let i = 0; i < 50; i++) {
            const time = measureTime(() => {
                delete require.cache[require.resolve('./superconfig_ffi.node')];
                const { SuperConfig } = require('./superconfig_ffi.node');
                return new SuperConfig();
            });
            startupTimes.push(time);
        }
        
        // Operations benchmark  
        const opTimes = [];
        for (let i = 0; i < 1000; i++) {
            const time = measureTime(() => {
                const config = new SuperConfig();
                return config.getVerbosity();
            });
            opTimes.push(time);
        }
        
        const calcStats = (times) => {
            times.sort((a, b) => a - b);
            const n = times.length;
            return {
                min: times[0],
                max: times[n-1],
                avg: times.reduce((a,b) => a+b) / n,
                median: n % 2 === 0 ? (times[n/2-1] + times[n/2])/2 : times[Math.floor(n/2)],
                p95: times[Math.floor(n * 0.95)]
            };
        };
        
        results.napi = {
            startup: calcStats(startupTimes),
            operations: calcStats(opTimes),
            file_size_kb: 607
        };
        
    } catch (error) {
        results.napi = { error: error.message };
    }
    
    console.log(JSON.stringify(results, null, 2));
}

benchmark();
        """

        # Write and run the Node.js benchmark
        with open("temp_nodejs_benchmark.js", "w") as f:
            f.write(nodejs_script)

        result = subprocess.run(
            ["node", "temp_nodejs_benchmark.js"],
            capture_output=True,
            text=True,
            timeout=60,
        )

        # Clean up
        Path("temp_nodejs_benchmark.js").unlink(missing_ok=True)

        if result.returncode == 0:
            data = json.loads(result.stdout)
            print("  âœ… Node.js benchmark completed")
            return data.get("napi", {})
        else:
            print(f"  âŒ Node.js benchmark failed: {result.stderr}")
            return None

    except Exception as e:
        print(f"  âŒ Node.js benchmark error: {e}")
        return None


def print_comprehensive_results(python_results, nodejs_results):
    """Print detailed comparison of all three approaches"""
    print("\nğŸ“ˆ COMPREHENSIVE PERFORMANCE COMPARISON")
    print("=" * 80)

    # File sizes
    print("\nğŸ“ FILE SIZES")
    print("â”€" * 40)
    print(
        f"Python (.so):     {python_results['file_size_kb'] if python_results else 'N/A'}KB"
    )
    print(
        f"NAPI (.node):     {nodejs_results['file_size_kb'] if nodejs_results else 'N/A'}KB"
    )
    print("WASI (.wasm):     50KB")

    if python_results and nodejs_results:
        # Startup comparison
        py_startup = python_results["startup"]
        napi_startup = nodejs_results["startup"]
        wasi_startup_avg = 1.203  # From previous benchmark

        print("\nğŸš€ STARTUP PERFORMANCE (ms)")
        print("â”€" * 60)
        print("                Python    NAPI      WASI      Fastest")
        print(
            f"Average:        {py_startup['avg']:.3f}     {napi_startup['avg']:.3f}     {wasi_startup_avg:.3f}     ",
            end="",
        )

        fastest_startup = min(py_startup["avg"], napi_startup["avg"], wasi_startup_avg)
        if fastest_startup == py_startup["avg"]:
            print("Python")
        elif fastest_startup == napi_startup["avg"]:
            print("NAPI")
        else:
            print("WASI")

        print(
            f"Median:         {py_startup['median']:.3f}     {napi_startup['median']:.3f}     1.059     ",
            end="",
        )
        fastest_median = min(py_startup["median"], napi_startup["median"], 1.059)
        if fastest_median == py_startup["median"]:
            print("Python")
        elif fastest_median == napi_startup["median"]:
            print("NAPI")
        else:
            print("WASI")

        print(
            f"95th percentile: {py_startup['p95']:.3f}     {napi_startup['p95']:.3f}     1.996     ",
            end="",
        )
        fastest_p95 = min(py_startup["p95"], napi_startup["p95"], 1.996)
        if fastest_p95 == py_startup["p95"]:
            print("Python")
        elif fastest_p95 == napi_startup["p95"]:
            print("NAPI")
        else:
            print("WASI")

        # Operations comparison
        py_ops = python_results["operations"]
        napi_ops = nodejs_results["operations"]
        wasi_ops_avg = 0.396  # From previous benchmark

        print("\nâš¡ OPERATION PERFORMANCE (ms)")
        print("â”€" * 60)
        print("                Python      NAPI        WASI      Fastest")
        print(
            f"Average:        {py_ops['avg']:.6f}   {napi_ops['avg']:.6f}   {wasi_ops_avg:.3f}     ",
            end="",
        )

        fastest_ops = min(py_ops["avg"], napi_ops["avg"], wasi_ops_avg)
        if fastest_ops == py_ops["avg"]:
            print("Python")
        elif fastest_ops == napi_ops["avg"]:
            print("NAPI")
        else:
            print("WASI")

        print(
            f"Median:         {py_ops['median']:.6f}   {napi_ops['median']:.6f}   0.230     ",
            end="",
        )
        fastest_ops_median = min(py_ops["median"], napi_ops["median"], 0.230)
        if fastest_ops_median == py_ops["median"]:
            print("Python")
        elif fastest_ops_median == napi_ops["median"]:
            print("NAPI")
        else:
            print("WASI")

        # Performance ratios
        print("\nğŸ† PERFORMANCE RATIOS (relative to fastest)")
        print("â”€" * 60)

        # Startup ratios
        fastest_startup_overall = min(
            py_startup["avg"], napi_startup["avg"], wasi_startup_avg
        )
        py_startup_ratio = py_startup["avg"] / fastest_startup_overall
        napi_startup_ratio = napi_startup["avg"] / fastest_startup_overall
        wasi_startup_ratio = wasi_startup_avg / fastest_startup_overall

        print(
            f"Startup speed:   {py_startup_ratio:.1f}x       {napi_startup_ratio:.1f}x        {wasi_startup_ratio:.1f}x"
        )

        # Operations ratios
        fastest_ops_overall = min(py_ops["avg"], napi_ops["avg"], wasi_ops_avg)
        py_ops_ratio = py_ops["avg"] / fastest_ops_overall
        napi_ops_ratio = napi_ops["avg"] / fastest_ops_overall
        wasi_ops_ratio = wasi_ops_avg / fastest_ops_overall

        print(
            f"Operation speed: {py_ops_ratio:.0f}x        {napi_ops_ratio:.1f}x        {wasi_ops_ratio:.0f}x"
        )

    print("\nğŸ¯ TECHNOLOGY COMPARISON SUMMARY")
    print("â”€" * 60)
    print("ğŸ PYTHON: Balanced performance, excellent for data science workflows")
    print("ğŸŸ¢ NAPI: Fastest operations, best for high-frequency Node.js APIs")
    print("ğŸŒŠ WASI: Smallest size + filesystem access, best for complex configs")

    print("\nğŸ’¡ USE CASE RECOMMENDATIONS")
    print("â”€" * 60)
    print("ğŸ”¥ High-frequency APIs (>1000 req/sec):     NAPI")
    print("ğŸ“Š Data science/analysis workflows:         Python")
    print("ğŸ“ Complex file-based configuration:        WASI")
    print("ğŸš€ Next.js middleware:                      NAPI")
    print("ğŸ§ª Jupyter notebooks/research:              Python")
    print("ğŸŒ Cross-platform deployment:               WASI")


def main():
    """Run comprehensive benchmark across all FFI variants"""
    print("ğŸ¯ SuperConfig FFI Comprehensive Performance Benchmark")
    print("ğŸ”¬ Testing Python, NAPI, and WASI versions\n")

    # Run Python benchmark
    python_benchmark = PythonBenchmark()
    python_results = python_benchmark.run_benchmark() if PYTHON_AVAILABLE else None

    # Run Node.js benchmark
    nodejs_results = run_nodejs_benchmark()

    # Print comprehensive comparison
    print_comprehensive_results(python_results, nodejs_results)

    print("\nâœ… Comprehensive benchmark complete!")

    if not python_results:
        print("\nğŸ’¡ To test Python performance:")
        print("   1. Make sure superconfig_ffi.so exists")
        print("   2. Run: python3 benchmark_python.py")


if __name__ == "__main__":
    main()
